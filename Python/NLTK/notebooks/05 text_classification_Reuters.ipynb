{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reuters-21578 collection\n",
    "\n",
    "Reuters-21578 is arguably the most commonly used collection for text classification during the last two decades.\n",
    "It has been used in some of the most influential papers on the field. \n",
    "\n",
    "This dataset contains structured information about newswire articles that can be assigned to several classes, \n",
    "therefore making this a multi-label problem. It has a highly skewed distribution of documents over categories, \n",
    "where a large proportion of documents belong to few topics.\n",
    "\n",
    "The collection originally consists of 21,578 documents, including documents without topics and typographical errors. \n",
    "For this reason, a subset and split of the collection is traditionally used. This split also focus only on the \n",
    "categories that have at least one document in the training set and the test set. The dataset has 90 categories with a \n",
    "training set of 7769 documents and a test set of 3019 documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Collection Stats\n",
    "\n",
    "Understanding your collection is always the first step of any data science problem. Lets have a quick look at the Reuters collection and its documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "# List of document ids\n",
    "documents = reuters.fileids()\n",
    "print(\"Documents: {}\".format(len(documents)))\n",
    "\n",
    "# Train documents\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "print(\"Total train documents: {}\".format(len(train_docs_id)))\n",
    "\n",
    "# Test documents\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
    "print(\"Total test documents: {}\".format(len(test_docs_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example of a document (with multiple labels)\n",
    "doc = 'training/9865'\n",
    "print(reuters.raw(doc))\n",
    "print()\n",
    "\n",
    "print(reuters.categories(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "\n",
    "# List of categories \n",
    "categories = reuters.categories();\n",
    "print(\"Number of categories: {}\".format(len(categories)))\n",
    "print()\n",
    "\n",
    "print(categories)\n",
    "print()\n",
    "\n",
    "# Documents per category.\n",
    "category_distribution = [(category, len(reuters.fileids(category))) \n",
    "                         for category in categories]\n",
    "\n",
    "category_distribution = sorted(category_distribution, \n",
    "                               key=itemgetter(1), \n",
    "                               reverse=True)\n",
    "\n",
    "print(\"Most common categories\")\n",
    "pprint(category_distribution[:5])\n",
    "print()\n",
    "\n",
    "print(\"Least common categories\")\n",
    "pprint(category_distribution[-5:])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
    "\n",
    "train_docs = [reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    "\n",
    "# Tokenisation \n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Learn and transform train documents\n",
    "vectorised_train_documents = vectorizer.fit_transform(train_docs)\n",
    "vectorised_test_documents = vectorizer.transform(test_docs)\n",
    "\n",
    "# Transform multilabel labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([reuters.categories(doc_id) \n",
    "                                  for doc_id in train_docs_id])\n",
    "test_labels = mlb.transform([reuters.categories(doc_id) \n",
    "                             for doc_id in test_docs_id])\n",
    "\n",
    "# Classifier \n",
    "classifier = OneVsRestClassifier(LinearSVC(random_state=42))\n",
    "classifier.fit(vectorised_train_documents, train_labels)\n",
    "predictions = classifier.predict(vectorised_test_documents)\n",
    "\n",
    "print(\"Number of labels assigned: {}\".format(sum([sum(prediction) \n",
    "                                                  for prediction in predictions])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well have we done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Show our quality\n",
    "precision = precision_score(test_labels, predictions, average='micro')\n",
    "recall = recall_score(test_labels, predictions, average='micro')\n",
    "f1 = f1_score(test_labels, predictions, average='micro')\n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, \n",
    "                                                                     recall, \n",
    "                                                                     f1))\n",
    "\n",
    "precision = precision_score(test_labels, predictions, average='macro')\n",
    "recall = recall_score(test_labels, predictions, average='macro')\n",
    "f1 = f1_score(test_labels, predictions, average='macro')\n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, \n",
    "                                                                     recall, \n",
    "                                                                     f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this weird warning message about \"Precision being ill-defined?\"\n",
    "\n",
    "Some metrics can be in a position of indeterminate value when for instance, the classifier decides to not classify any articles in a specific category. This would imply a 0/0 precision. Scikit learn warns us about this, and reports back this quality as 0.0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
