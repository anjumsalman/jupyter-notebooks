{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b3ad96-c7d2-4e1e-819d-4de533caab93",
   "metadata": {},
   "source": [
    "## Key Value Store\n",
    "A non-relational database which supports two operations - `get(key)` and `put(key, value)` to retrieve and store key-value pairs. Some examples include Redis, Memcached, Amazon Dynamo, etc.\n",
    "\n",
    "Key needs to be hashable and the value is any object. Some design considerations:\n",
    "- should be scalable\n",
    "- choice between consistency and availability (tunable consistency)\n",
    "- low latency\n",
    "\n",
    "## Single Server Store\n",
    "Implementation of a single server store is relatively simple - use a in-memory hashtable. However, memory is limited and we'll soon fill it up. One approach to mitigate the problem is to have an LRU cache and evict key-value pair to disk. Additionally, we can try compressing values.\n",
    "\n",
    "If a key-value pair is not stored in disk, the `get` operation becomes slow since it has to fetch data from disk. To support large amount of data, we need to shift to using a distributed approach.\n",
    "\n",
    "## Distributed Key-Value Store\n",
    "Key-value pairs are distributed over multiple machines. In a distributed system, CAP theorem applies, therefore we can guarantee either consistency or availability. As an example, let's consider a distributed system consisting of 3 nodes  $n_1$, $n_2$ and $n_3$. And $n_3$ goes down.  \n",
    "**Consistency:** if we chooose consistency, we block read and writes from $n_1$ and $n_2$ till $n_3$ is back online. The service returns error.  \n",
    "**Availability:** if we choose availability, the system keeps accepting reads, even though it might return stale data. For writes, $n_1$ and $n_2$ will keep accepting writes, and data will be synced to $n_3$ when the network partition is resolved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aadcf38-3d24-4ad9-bb0a-4170ee8642a0",
   "metadata": {},
   "source": [
    "### Partitioning\n",
    "One server can't possibly store all the data, so we split data into smaller partitions and each node stores a fraction of the total data. Consistent hasing allows us to:\n",
    "- spread data evenly across multiple nodes\n",
    "- minimise data movement when nodes are added/removed.\n",
    "\n",
    "<img src=\"images/key_value_consistent_hash.png\" />\n",
    "\n",
    "Any of the nodes can receive request from the client. The node receiving the request is referred to as coordinator. The coordinator calculates hash and then forwards the request to the right node.\n",
    "\n",
    "The number of virtual nodes for a server is proportional to the server capacity. For example, servers with higher capacity are assigned with more virtual nodes.\n",
    "\n",
    "### Replication\n",
    "To achieve high availability and reliability, data must be replicated asynchronously over $N$ servers, where $N$ is a configurable parameter called as *replication factor*. For example, if replication factor is set to 3, here is how a `put` operation works:  \n",
    "\n",
    "<img src=\"images/key_value_replication.png\"/>\n",
    "\n",
    "For better reliability, replicas are placed in distinct data centers, and data centers are connected through high-speed networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2d085f-1601-4102-9ee4-fb7928d3be26",
   "metadata": {},
   "source": [
    "### Consistency\n",
    "Let there be a distributed system with replication factor $N$. For a write operation to be successful, it must be acknowledged by $W$ number of replicas. Similarly, for a read operation to be successful, it must be acknowledged by $R$ number of replicas.  \n",
    "\n",
    "As an example, setting $W=1$ means we get fast writes since acknowledgement from just one node is enough. The system also has high availability (at the cost of consistency). Configuration of $W$, $R$ and $N$ is a typical tradeoff between latency and consistency. Some configurations:\n",
    "- $R= 1$ and $W=N$, the system is optimized for a fast read.\n",
    "- $W=1$ and $R=N$, the system is optimized for fast write.\n",
    "- $W+R>N$, strong consistency is guaranteed (Usually $N=3, W=R=2$).\n",
    "- $W+R<=N$, strong consistency is not guaranteed.\n",
    "\n",
    "**Strong Consistency:** to ensure strong consistency, $W=N$. Additionally, concurrent writes for the same key should not be allowed.  \n",
    "**Weak consistency:** subsequent read operations may not see the most updated value.\n",
    "\n",
    "<img src=\"images/key_value_inconsistent.png\" />\n",
    "\n",
    "**Eventual consistency:** this is a specific form of weak consistency. Given enough time, all updates are propagated, and all replicas are consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72004dc7-bcc4-47b8-9d2a-089b33b06275",
   "metadata": {},
   "source": [
    "### Inconsistency Resolution\n",
    "One way to resolve inconsistency is to maintain a timestamp alongwith every write operation. On a single machine, maintaining sequence of events is easy - maintain timestamp of each event. The same physical clock is used for all timestamps providing a global view of time.\n",
    "\n",
    "When we move to distributed system, we cannot reply on this property anymore. We cannot guarantee that each machine's clock has the same exact time. Therefore relying on the last timestamp to resolve inconsistency (last write wins), is not the best approach.\n",
    "\n",
    "**Vector Clock** [explanation](https://riak.com/posts/technical/vector-clocks-revisited/index.html?p=9545.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123fa228-58ab-467c-9087-c36375b2c299",
   "metadata": {},
   "source": [
    "### Failure Detection\n",
    "To know which nodes are UP, every node sends heartbeats to every other node in the system. However, this is inefficient when many servers are in the system - requires $O(n^2)$ number of heartbeats.\n",
    "\n",
    "**Gossip Protocol:** is a decentralized peer-to-peer communication technique to transmit messages in an enormous distributed system. Gossip protocol is simple in concept. Each node sends out some data to a set of other nodes. Data propagates through the system node by node like a virus. Eventually data propagates to every node in the system. It's a way for nodes to build a global map from limited local interactions.\n",
    "\n",
    "Every node also supports the following two REST operations: i) GET information about all nodes ii) POST operation that updates all node information sent by another node (through heartbeat). There is also a timer task executed by a node which:\n",
    "- increments its own heartbeat counter\n",
    "- moves the peers it hasn’t had a heartbeat exceeding the threshold to “suspected” state\n",
    "- removes the peers in suspected state which haven’t had any heartbeats exceeding the threshold\n",
    "- randomly pick $n$ peers to send this node’s membership list to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d813a7df-9f6e-4c72-a91f-f1ed96572c08",
   "metadata": {},
   "source": [
    "### Hinted Handoff\n",
    "When a server goes down, unlike in a strongly consistent system where read and writes are blocked, in an available system, the first $W$ healthy servers for writes and first $R$\n",
    "healthy servers for reads are chosen on the hash ring. Offline servers are ignored.\n",
    "\n",
    "If a server is unavailable due to network or server failures, another server will process requests temporarily. When the down server is up, changes will be pushed back to achieve data consistency. This process is called *hinted handoff*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b33decf-5f85-4dda-b65a-e01dc6c53bab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "21.0.2+13-58"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
