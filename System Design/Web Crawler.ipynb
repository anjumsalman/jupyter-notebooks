{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76dbe7ef-1e00-4397-a2f3-1ddee7179ac9",
   "metadata": {},
   "source": [
    "## Overview\n",
    "A web crawler begins by collecting a set of webpages and then following links on those pages to collect new pages. It is used for the following purpose:\n",
    "- indexing search engines as in the case of Google, Bing, etc.\n",
    "- archiving webpages like in the case of Archive.org\n",
    "- web mining to collect useful information from web\n",
    "\n",
    "## Calculations\n",
    "Lets say the crawler has to collect 1 billion pages a month. This means that it has to collect $\\frac{1,000,000,000}{30 \\times 24 \\times 3600} = 400$ pages a second. If the peak QPS is double than average, then it would be $800$ pages a second.\n",
    "\n",
    "Assuming that we need to save webpages, and average size of a webpage being $500 KB$ in size, this means we need $\\frac{1,000,000,000 \\times 500}{1024 \\times 1024 \\times 1024} = 465 TB$ of storage per month. In 5 years this would account for around $30 PB$.\n",
    "\n",
    "## Architecture\n",
    "![Crawler Architecture](./images/crawler_architecture.png)\n",
    "\n",
    "### Seed URL\n",
    "A good seed URL serves as a good starting point that a crawler can utilize to traverse as many links as possible. There can be multiple different approaches of selecting a good set of seed URLs:\n",
    "- categorise links geographically\n",
    "- categorise links by topics, etc\n",
    "\n",
    "### URL Frontier\n",
    "Maintains list of URLs to crawl. A simple implementation would be FIFO queue, however it will not satisfy some properties expected of frontiers.\n",
    "\n",
    "**Politness:** Most hyperlinks on the web are “relative” (i.e., refer to another page on the same web server). Frontier realized\n",
    "as a FIFO queue contains long runs of URLs referring to pages on the same web server, resulting in the crawler issuing many consecutive HTTP requests to that server, leading to potential denial of service attack.\n",
    "\n",
    "One way to achieve this is my maintaining separate queue per crawler (downloader) thread (or machine). The domain part of URL is hashed and then the URL is stored in a queue. This way only one queue would contain all URLs of a domain, thereby slowing down requests to that domain.\n",
    "\n",
    "![Politeness](./images/politeness.png)\n",
    "\n",
    "**Priority:** a good web crawler associates a priority with web pages based on the pages usefulness. Higher priority means more likely to be downloaded. There can be multiple ways to determine priority - page traffic, rate of change, *PageRank*, etc.\n",
    "\n",
    "### Storage for Frontier\n",
    "The best place to store frontier data would be memory, however it would soon fill up. It can be stored on disk, but then it will be too slow to access. A hybrid approach is taken where majority of URLs are stored on disk. A small buffer is maintained in memory for enqueue/dequeue operations.\n",
    "\n",
    "## Downloader\n",
    "Downloads webpages from the internet based on the URL provided by the frontier. Downloader keeps in consideration the *robots.txt* file which specifies what pages a crawler is permitted to download. Snippet of contents of robots.txt for apple.com:\n",
    "```\n",
    "User-agent: *\n",
    "Disallow: /*shop/browse/overlay/*\n",
    "Disallow: /*shop/iphone/payments/overlay/*\n",
    "Disallow: /cn/*/aow/*\n",
    "Disallow: /tmall*\n",
    "Allow: /ac/globalnav/2.0/*/images/ac-globalnav/globalnav/search/* \t\n",
    "```\n",
    "\n",
    "It is good practise to cache contents of this file. To improve upon the performance of download:\n",
    "- **Distributed Crawl:** crawl jobs are distributed into multiple servers, and each server runs multiple threads. The URL space is partitioned into smaller pieces; so, each downloader is responsible for a subset of the URLs.\n",
    "- **DNS Cache:** maintain local DNS cache as repeated DNS queries can slow down the process\n",
    "- **Geographical Distribution:** crawler geographically closer to a server can download content faster, so distributing downloader globally can speed up the process.\n",
    "- **Short Timeout:** set short timeouts. This would prevent slow websites from being bottleneck.\n",
    "\n",
    "The robustess of crawling process can be improved by:\n",
    "- **Consistent Hashing:** having consistent hash based architecture such that new downloaders can be added and removed without redistributing all the URLs to be downloaded. This process needs to be complemented with a storage system to save crawl state.\n",
    "- **Exception Handling:** downloaders should gracefully handle any exception generated during the process. Some pages called as *spider traps* can lead to infinite loop by creating an infinite directory structure. Example. example.com/foo/bar/foo/bar/foo... One way to detect this is by limiting URL length.\n",
    "\n",
    "\n",
    "## Content Parser\n",
    "Downloaded webpages need to be parsed and validated for correctness. It is often a separate component as compared to downloader to prevent slowing down the downloader.\n",
    "\n",
    "## Content Seen\n",
    "Since lot of the web contains duplicate content, it makes sense to check for whether the content has already been stored. This can be done by generating hash of the content and comparing it with hash of existing content.\n",
    "\n",
    "## Link Extractor and Filter\n",
    "Link extractor retrieves URLs from webpages. Since many links are relative, these are converted into absolute URLs by adding the domain of the current page. The URL filter excludes certain content types, file extensions, error links and URLs in blacklisted problematic domains.\n",
    "\n",
    "## URL Seen\n",
    "It is a data structure that keeps track of URLs that are visited before or already in the Frontier. It helps to avoid adding the same URL multiple times as this can increase server load and cause potential infinite loops.\n",
    "*Bloom filter* and *hash table* are common techniques to implement the URL Seen? component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b53b78-ad5d-40c9-9c1e-4da6c7953f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "21.0.2+13-58"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
