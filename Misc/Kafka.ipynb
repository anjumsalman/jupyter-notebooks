{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faed2fb5-0182-4ec0-8170-eb6aa8da4f8e",
   "metadata": {},
   "source": [
    "## Publisher/Subscriber System\n",
    "Pattern that is characterized by the sender (publisher) of a piece of data (message) not specifically directing it to a receiver. Pub/sub systems often have a broker, a central point where messages are published, to facilitate this.  \n",
    "\n",
    "Kafka is pub/sub system. Data within Kafka is stored *durably*, in *order*, and can be read *deterministically*. In addition, the data can be *distributed* within the system to provide additional protections against failures, as well as significant opportunities for *scaling* performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5442ad0-afb6-4d41-9752-4c5ce96178c2",
   "metadata": {},
   "source": [
    "## Terminologies\n",
    "**Message:**  unit of data within Kafka - similar to a row in database terms. It doesn't have any special format, it is just an array of bytes.  \n",
    "**Key:** optional bit of metadata associated with a message. Key is also a byte array and, as with the message, has no specific meaning to Kafka. Keys are used when messages are to be written to partitions in a more controlled manner. The simplest such scheme is to generate a consistent hash of the key, and then select the partition number for that message by taking the result of the hash modulo, the total number of partitions in the topic. This assures that messages with the same key are always written to the same\n",
    "partition.  \n",
    "**Batch:** collection of messages being produced to the same topic and message. This increases throughput with some penalty on latency.  \n",
    "**Topic:** messages are categorized into topics. Analogous to a table in database.  \n",
    "**Partition:** topic is broken down into a number of partitions. As a topic typically has multiple partitions, there is no guarantee of message time-ordering across the entire topic, just within a single partition. Partitions are also the way that Kafka provides redundancy and scalability. Each partition can be hosted on a different server, which means that a single topic can be scaled horizontally across multiple servers.  \n",
    "**Producers:** create new messages destined to a specific topic. By default producer doesn't care which partition the message is added to, however facilities exist to send messages to specific partition using keys.  \n",
    "**Consumers:** read message in the order they were produced. Consumer keeps track of which messages it has already consumed by keeping track of the offset of message.  \n",
    "**Offset:** another bit of metadata—an integer value that continually increases—that Kafka adds to each message as it is produced. Each message in a given partition has a unique offset, and Kafka can store offset for each partition.  \n",
    "**Consumer group:** one or more consumers that work together to consume a topic. The group assures that each partition is only consumed by one member.  \n",
    "\n",
    "<img src=\"./images/partitions_consumer_groups.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a84506-e570-4201-80fa-fbbad8ac03fd",
   "metadata": {},
   "source": [
    "**Broker:** a single Kafka server is called a broker. Broker receives messages from producers, assigns offsets to them, and commits the messages to storage on disk. It also services consumers, responding to fetch requests for partitions and responding with the messages that have been committed to disk.  \n",
    "**Cluster:** brokers are part of a cluster. Among the brokers, one will be the leader broker called as controller. It is responsible for assigning partitions to brokers and monitoring for broker failures. A partition is owned by a single broker in the cluster, and that broker is called the leader of the partition. A partition may be assigned to multiple brokers, which will result in the partition being replicated, thus providing redundancy. All consumers and producers operating on that partition, however must connect to the leader.  \n",
    "\n",
    "<img src=\"./images/cluster_broker_replication.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b56076-51fe-443e-8c45-ee9ed8af87fb",
   "metadata": {},
   "source": [
    "## Partition Considerations\n",
    "Usually the number of partitions for a topic is chosen to be equal to the number of brokers in the cluster. There are several factors to consider while determining the correct number of partitions:\n",
    "- Expected throughput on the topic. 100KB or 1GB per second?\n",
    "- Maximum throughput expected from a single partition? At any time, we can have a maximum of one consumer reading from a partition. So in a way the throughput would be limited by the slowest consumer\n",
    "- If we are sending messages to partitions based on keys, adding partition later can be very challenging.\n",
    "- Consider number of partitions being placed on each broker in terms of diskspace and network bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f8d332-f950-40bf-80de-e2b52ab150dd",
   "metadata": {},
   "source": [
    "## Producer\n",
    "Flow:  \n",
    "<img src=\"./images/producer_flow.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e41f2d-1803-4a74-a9fa-100966ac8f27",
   "metadata": {},
   "source": [
    "### Creating Producer\n",
    "Producer object facilitates thread safe pulishing of messages to Kafka. To instantiate a producer object, we need the following mandatory properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7465e4f5-8dfd-4a44-9ed5-b7b52d5a7501",
   "metadata": {},
   "outputs": [],
   "source": [
    "private Properties kafkaProps = new Properties();\n",
    "kafkaProps.put(\"bootstrap.servers\", \"broker1:9092,broker2:9092\");\n",
    "kafkaProps.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n",
    "kafkaProps.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n",
    "KafkaProducer<String, String> producer = new KafkaProducer<>(kafkaProps);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dada30fc-4ebf-4eeb-9838-d888c6551a6a",
   "metadata": {},
   "source": [
    "Full list of producer configuration properties [here](https://kafka.apache.org/documentation.html#producerconfigs). However some important ones are listed below:\n",
    "\n",
    "| Property                  | Description                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
    "|---------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| acks                      | acks=0, producer will not wait for a reply from the broker before assuming the message was sent successfully. High throughput, but producer will not know about any failures. acks=1, producer will receive a success response from the broker the moment the leader replica received the message. acks=all, producer will receive a success response from the broker once all in-sync replicas received the message. High latency. |\n",
    "| buffer.memory             | sets the amount of memory the producer will use to buffer messages waiting to be sent to brokers. If messages are sent by the application faster than they can be delivered to the server, the producer may run out of space and additional send() calls will either block or throw an exception, based on the `block.on.buffer.full` prop.                                                                                         |\n",
    "| compression.type          | can be set to snappy, gzip, or lz4.                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
    "| retries                   | how many times the producer will retry sending the message before giving up and notifying the client of an issue. Producer will wait `retry.backoff.ms` amount of time before retrying.                                                                                                                                                                                                                                             |\n",
    "| client.id                 | used by broker to identify client                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
    "| timeout.ms                | controls the time the broker will wait for in-sync replicas to acknowledge the message.                                                                                                                                                                                                                                                                                                                                             |\n",
    "| metadata.fetch.timeout.ms | how long the producer will wait for a reply from the server when sending data.                                                                                                                                                                                                                                                                                                                                                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac0a57-0b86-479b-a1cb-f20c48349613",
   "metadata": {},
   "source": [
    "### Sending Message\n",
    "A message can be sent by producer in the following ways:\n",
    "1. Fire and forget\n",
    "2. Synchronously\n",
    "3. Asynchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f45df-0fb6-458c-a7c9-80dcb9e6b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Fire and forget\n",
    "ProducerRecord<String, String> record = \n",
    "    new ProducerRecord<>(\"CustomerCountry\", \"Precision Products\", \"France\");\n",
    "    // <Topic>,<Key>,<Value>\n",
    "\n",
    "try {\n",
    "    producer.send(record);  // Send returns Future<RecordMetadata>\n",
    "} catch(Exception e) {\n",
    "    logger.error(e);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f08faa-43db-4ba9-953f-b5e956a36c5e",
   "metadata": {},
   "source": [
    "There are some errors that can happen before sending message to Kafka. Those can be `SerializationException`, `BufferExhaustedException` or `TimeoutException` if buffers are full. Or `InterruptionException` if the sending thread is interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3846fffa-0dcf-4ef4-9be8-917bb09f5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Synchronous send\n",
    "try {\n",
    "    producer.send(record).get();\n",
    "} catch(Exception e) {\n",
    "    logger.error(e);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a95e6-e61c-48eb-81c6-f9bd185bf46e",
   "metadata": {},
   "source": [
    "Some errors are retriable, whereas others aren't. Kafka producer automatically retries in case of retriable exceptions. Examples of retriable exception would be connection error or a \"no leader\" error. Non retriable error would be something like message size being too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e39f7-fc29-4bf8-aaa9-b05793909bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Asynchronous send\n",
    "producer.send(record, (m, e)->{\n",
    "    if(e != null){\n",
    "        logger.error(e);\n",
    "    }\n",
    "});\n",
    "\n",
    "// The second argument is a org.apache.kafka.clients.producer.Callback interface object\n",
    "// which has one method public void onCompletion(RecordMetadata recordMetadata, Exception e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346671b9-7276-4991-82df-06bd351f8f34",
   "metadata": {},
   "source": [
    "### Partitions\n",
    "Kafka messages are key-value pairs and while it is possible to create a `ProducerRecord` with just a topic and a value, with the key set to null by default, most applications produce records with keys. Keys serve two goals: they are additional information that gets stored with the message, and they are also used to decide which one of the topic partitions the message will be written to. All messages with the same key will go to the same partition.  \n",
    "\n",
    "When the key is null and the default partitioner is used, the record will be sent to one of the available partitions of the topic at random. A round-robin algorithm will be used to balance the messages among the partitions.  \n",
    "\n",
    "The mapping of keys to partitions is consistent only as long as the number of partitions in a topic does not change. However, the moment you add new partitions to the topic, this is no longer guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0306a59-cdbd-4b54-902a-f522ec1938d4",
   "metadata": {},
   "source": [
    "## Consumers\n",
    "Applications that need to read data from Kafka use a `KafkaConsumer` to subscribe to Kafka topics and receive messages from these topics. Kafka consumers are typically part of a consumer group. When multiple consumers are subscribed to a topic and belong to the same consumer group, each consumer in the group will receive messages from a different subset of the partitions in the topic. Here is how consumer groups get mapped to partitions: \n",
    "\n",
    "<img src=\"./images/cg_scaling.png\" />  \n",
    "\n",
    "We can have multiple consumer groups consuming the same topic. Each will get messages in the topic independent of one another.  \n",
    "\n",
    "<img src=\"./images/multi_cg.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d00fd-93dc-4bf1-9016-152bf194623f",
   "metadata": {},
   "source": [
    "### Partition Rebalance\n",
    "When we add a new consumer to the group, it starts consuming messages from partitions previously consumed by another consumer. The same thing happens when a consumer shuts down or crashes; it leaves the group, and the partitions it used to consume will be consumed by one of the remaining consumers. Reassignment of partitions to consumers also happen when the topics the consumer group is consuming are modified (e.g., if an administrator adds new partitions).  \n",
    "\n",
    "During a rebalance, consumers can’t consume messages, so a rebalance is basically a short window of unavailability of the entire consumer group.  \n",
    "\n",
    "The way consumers maintain membership in a consumer group and ownership of the partitions assigned to them is by sending heartbeats to a Kafka broker designated as the *group coordinator* (this broker can be different for different consumer groups)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f060198-0405-41ad-a908-6f3671aee68f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating Consumers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9858026-ef16-4af7-8090-c2a503d7071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Properties props = new Properties();\n",
    "props.put(\"bootstrap.servers\", \"broker1:9092,broker2:9092\");\n",
    "props.put(\"group.id\", \"CountryCounter\"); // This is the consumer group (not mandatory)\n",
    "props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n",
    "props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n",
    "KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f085d56-f1ed-49e8-8d3a-ecf73bb149ee",
   "metadata": {},
   "source": [
    "### Subscribing to Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5767a58f-8f0b-46b0-8ceb-84b73a8f257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer.subscribe(Collections.singletonList(\"customerCountries\"));\n",
    "\n",
    "// Polling for messages - handles all details of coordination,\n",
    "// partition rebalances, heartbeats, and data fetching\n",
    "try {\n",
    "    while (true) {\n",
    "        // argument is timeout interval and controls how long poll() will block if data is not available\n",
    "        // in the consumer buffer. Set to 0, poll() will return immediately\n",
    "        ConsumerRecords<String, String> records = consumer.poll(100);\n",
    "        \n",
    "        // Poll returns multiple records. Each record contains the topic and partition the\n",
    "        // record came from, the offset of the record within the partition, and of course the\n",
    "        // key and the value of the record.\n",
    "        for (ConsumerRecord<String, String> record : records) {\n",
    "            log.debug(\"topic = %s, partition = %s, offset = %d, customer = %s, country = %s\\n\",\n",
    "                record.topic(), record.partition(), record.offset(), record.key(), record.value());\n",
    "            \n",
    "            int updatedCount = 1;\n",
    "            if (custCountryMap.countainsValue(record.value())) {\n",
    "                updatedCount = custCountryMap.get(record.value()) + 1;\n",
    "            }\n",
    "            \n",
    "            custCountryMap.put(record.value(), updatedCount);\n",
    "            JSONObject json = new JSONObject(custCountryMap);\n",
    "            System.out.println(json.toString(4));\n",
    "        }\n",
    "    }\n",
    "} finally {\n",
    "    // This will close the network connections and sockets.\n",
    "    // Also retrigger rebalance immediately.\n",
    "    consumer.close();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9e2a4d-e90d-4d9d-bebd-2ada2eeb0979",
   "metadata": {},
   "source": [
    "The poll loop does a lot more than just get data. The first time we call `poll()` with a new consumer, it is responsible for finding the *GroupCoordinator*, joining the consumer group, and receiving a partition assignment. If a rebalance is triggered, it will\n",
    "be handled inside the poll loop as well. And of course the heartbeats that keep consumers alive are sent from within the poll loop.  \n",
    "\n",
    "One consumer per thread is the rule. To run multiple consumers in the same group in one application, we will need to run each in its own thread."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21795d4-52cb-415c-9c03-516fba1c4094",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Consumer Configurations\n",
    "Full list available [here](https://kafka.apache.org/documentation.html#newconsumerconfigs). Some important ones are: \n",
    "\n",
    "| Property           | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
    "|--------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| fetch.min.bytes    | minimum amount of data that it wants to receive from the broker when fetching records. If a broker receives a request for records from a consumer but the new records amount to fewer bytes, the broker will wait until more messages are available before sending the records back to the consumer.                                                                                                                                                                                                                       |\n",
    "| fetch.max.wait.ms  | lets us control how long to wait. By default, Kafka will wait up to 500 ms.                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
    "| session.timeout.ms | amount of time a consumer can be out of contact with the brokers while still considered alive defaults to 3 seconds. Its value should be greater than `heartbeat.interval.ms`                                                                                                                                                                                                                                                                                                                                              |\n",
    "| auto.offset.reset  | Controls the behavior of the consumer when it starts reading a partition for which it doesn’t have a committed offset or if the committed offset it has is invalid (the consumer was down for so long that the record with that offset was already aged out of the broker). latest: default value and means that lacking a valid offset, the consumer will start reading from the newest records earliest: lacking a valid offset, the consumer will read all the data in the partition, starting from the very beginning. |\n",
    "| max.poll.records   | maximum number of records that a single call to `poll()` will return.                                                                                                                                                                                                                                                                                                                                                                                                                                                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f91119-3f43-46c1-b8d7-00147dc56c41",
   "metadata": {},
   "source": [
    "### Commits and Offsets\n",
    "Kafka differs from other messaging system like JMS in that it does not track acknowledgments from consumers. Consumers are responsible for tracking their position in each partition. The action of updating the current position in the partition a **commit**. Consumers commit an offset by regularly commiting their offset by producing message to a special topic named `__consumer_offsets`.  \n",
    "\n",
    "Normally, the client library maintains the last read offset in memory and when a client polls, the client library gives the app the next messages from its own client side buffer which it continues to fill in the background (if needed) with fetch requests to the brokers. However that mechanism is independent of committed offsets and works even if the app never commits an offset to the `__consumer-offsets` topic. Committing offsets is like a checkpoint for where you want the app to return should it fail.\n",
    "\n",
    "In this setting everything works well in case all consumers are up and working fine, however in case of a consumer crash, rebalance happens and the new consumer needs to know where to start consuming new message from. The consumer will read the latest committed offset of each partition and continue from there.  \n",
    "\n",
    "This can lead situations where:\n",
    "- messages between the last processed offset and the committed offset will be processed twice.\n",
    "- OR committed offset is larger than the offset of the last message the client actually processed, all messages between the last processed offset and the committed offset will be missed by the consumer group.\n",
    "\n",
    "**Automatic Commits:** configure `enable.auto.commit=true`, then every five seconds the consumer will commit the largest offset your client received from `poll()`. Just like everything else in the consumer, the automatic commits are driven by the poll loop. Whenever\n",
    "we poll, the consumer checks if it is time to commit, and if it is, it will commit the offsets it returned in the last poll. This configuration can however lead to duplicate processing of messages.  \n",
    "\n",
    "**Commit Current Offset:** By setting `auto.commit.offset=false`, offsets will only be committed when the application explicitly chooses to do so (using `commitSync()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f3f788-cb3d-4476-8049-2765bdb30604",
   "metadata": {},
   "outputs": [],
   "source": [
    "while (true) {\n",
    "    ConsumerRecords<String, String> records = consumer.poll(100);\n",
    "    for (ConsumerRecord<String, String> record : records) {\n",
    "        System.out.printf(\"topic = %s, partition = %s, offset = %d, customer = %s, country = %s\\n\",\n",
    "            record.topic(), record.partition(), record.offset(), record.key(), record.value());\n",
    "    }\n",
    "    \n",
    "    try {\n",
    "        consumer.commitSync(); // commits the latest offset returned by the last poll()\n",
    "    } catch (CommitFailedException e) {\n",
    "        log.error(\"commit failed\", e)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3773e3d7-7884-49de-827e-3e9f85f6e0d2",
   "metadata": {},
   "source": [
    "**Commit Asynchronously:** to improve throughput of the application. The drawback is that while `commitSync()` will retry the commit until it either succeeds or encounters a nonretriable failure, `commitAsync()` will not retry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a1ebac-10f3-47e8-b291-73bc95a954c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "while (true) {\n",
    "    ConsumerRecords<String, String> records = consumer.poll(100);\n",
    "    for (ConsumerRecord<String, String> record : records) {\n",
    "        System.out.printf(\"topic = %s, partition = %s, offset = %d, customer = %s, country = %s\\n\",\n",
    "        record.topic(), record.partition(), record.offset(), record.key(), record.value());\n",
    "    }\n",
    "    \n",
    "    consumer.commitAsync();\n",
    "}\n",
    "\n",
    "// Commit async supports callback\n",
    "consumer.commitAsync(new OffsetCommitCallback() {\n",
    "    public void onComplete(Map<TopicPartition, OffsetAndMetadata> offsets, Exception exception) {\n",
    "    if (e != null)\n",
    "        log.error(\"Commit failed for offsets {}\", offsets, e);\n",
    "    }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47deef5-6f55-4e42-a29e-f569858d75c7",
   "metadata": {},
   "source": [
    "A simple pattern to get commit order right for asynchronous retries is to use a monotonically increasing sequence number. Increase the sequence number every time we commit and add the sequence number at the time of the commit to the `commitAsync` callback. When we’re getting ready to send a retry, check if the commit sequence number the callback got is equal to the instance variable; if it is, there was no newer commit and it is safe to retry. If the instance sequence number is higher, don’t retry because a newer commit was already sent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1cb12d-af68-4849-851f-3266292107da",
   "metadata": {},
   "source": [
    "**Combined:** A common pattern is to combine `commitAsync()` with `commitSync()` just before shutdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f17bbe-26b0-4ab3-a612-3575c849089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try {\n",
    "    while (true) {\n",
    "        ConsumerRecords<String, String> records = consumer.poll(100);\n",
    "        for (ConsumerRecord<String, String> record : records) {\n",
    "            System.out.printf(\"topic = %s, partition = %s, offset = %d, customer = %s, country = %s\\n\",\n",
    "            record.topic(), record.partition(), record.offset(), record.key(), record.value());\n",
    "        }\n",
    "        consumer.commitAsync();\n",
    "    }\n",
    "} catch (Exception e) {\n",
    "    log.error(\"Unexpected error\", e);\n",
    "} finally {\n",
    "    try {\n",
    "        consumer.commitSync();\n",
    "    } finally {\n",
    "        consumer.close();\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ff37a-04de-484e-b3a2-d0324ac7896e",
   "metadata": {},
   "source": [
    "What if the number of records received in poll is high and we want to commit in between processing? The following code commits after every 1000 records processed using an alternate form of `commitAsync`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f65a13-a518-4d4b-9399-65bc95901804",
   "metadata": {},
   "outputs": [],
   "source": [
    "private Map<TopicPartition, OffsetAndMetadata> currentOffsets = new HashMap<>();\n",
    "int count = 0;\n",
    "// ...\n",
    "\n",
    "while (true) {\n",
    "    ConsumerRecords<String, String> records = consumer.poll(100);\n",
    "    for (ConsumerRecord<String, String> record : records) {\n",
    "        System.out.printf(\"topic = %s, partition = %s, offset = %d, customer = %s, country = %s\\n\",\n",
    "            record.topic(), record.partition(), record.offset(), record.key(), record.value());\n",
    "    \n",
    "        currentOffsets.put(new TopicPartition(record.topic(), record.partition()), \n",
    "            new OffsetAndMetadata(record.offset()+1, \"no metadata\"));\n",
    "        if (count % 1000 == 0)\n",
    "            consumer.commitAsync(currentOffsets, null);\n",
    "        count++;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855427c2-b863-48d7-8fe8-85de01602f83",
   "metadata": {},
   "source": [
    "**Consuming Records with a Paricular Offset:** If we want to start reading all messages from the beginning of the partition, or we\n",
    "want to skip all the way to the end of the partition and start consuming only new messages, there are APIs specifically for that: `seekToBeginning(TopicPartition tp)` and `seekToEnd(TopicPartition tp)`.  \n",
    "\n",
    "However, consider the following case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c539c-5cc1-4b14-adef-dff7ef91fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "while (true) {\n",
    "    ConsumerRecords<String, String> records = consumer.poll(100);\n",
    "    for (ConsumerRecord<String, String> record : records) {\n",
    "        currentOffsets.put(new TopicPartition(record.topic(), record.partition()), record.offset());\n",
    "        processRecord(record);\n",
    "        storeRecordInDB(record);\n",
    "        // failure happens here, we are unable to commit latest offset\n",
    "        consumer.commitAsync(currentOffsets);\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452e8dc-7cef-4a27-83e8-b1a61f18b164",
   "metadata": {},
   "source": [
    "This could be avoided if there was a way to store both the record and the offset in one atomic action. Either both the record and the offset are committed, or neither of them are committed - using perhaps database transactions. The offset is now somehow stored in database instead of Kafka. We can use `seek` in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545dc6fd-1e2a-4709-9578-09ebe84babe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "public class SaveOffsetsOnRebalance implements ConsumerRebalanceListener {\n",
    "    public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n",
    "        commitDBTransaction();\n",
    "    }\n",
    "    public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n",
    "        for(TopicPartition partition: partitions) {\n",
    "            consumer.seek(partition, getOffsetFromDB(partition));\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "consumer.subscribe(topics, new SaveOffsetOnRebalance(consumer));\n",
    "// we call poll() once to make sure we join a consumer group and get assigned partitions\n",
    "consumer.poll(0);\n",
    "\n",
    "for (TopicPartition partition: consumer.assignment()) \n",
    "    consumer.seek(partition, getOffsetFromDB(partition));\n",
    "\n",
    "while (true) {\n",
    "    ConsumerRecords<String, String> records = consumer.poll(100);\n",
    "    for (ConsumerRecord<String, String> record : records) {\n",
    "        processRecord(record);\n",
    "        storeRecordInDB(record);\n",
    "        storeOffsetInDB(record.topic(), record.partition(), record.offset());\n",
    "    }\n",
    "    // database records and offsets will be inserted to the database when we commit transaction\n",
    "    commitDBTransaction();\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "17.0.5+9-LTS-191"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
