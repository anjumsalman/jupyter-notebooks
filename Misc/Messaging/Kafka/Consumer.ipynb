{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee3dd72-4a9d-437d-9f04-7b2ea0e9ae39",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Message written to a topic by a producer is read by one or more Kafka consumers. Just as there can be multiple producers, there can be multiple consumers. Consumers are part of a *consumer group*. A partition of a topic is assigned to a unique consumer of a consumer group. The distribution looks like:\n",
    "\n",
    "<img src=\"images/cg_scaling.png\" width=900 height=auto />\n",
    "\n",
    "We can have multiple consumer groups consuming the same topic. Each will get messages in the topic independent of one another:\n",
    "\n",
    "<img src=\"images/multi_cg.png\" width=350 height=auto />\n",
    "\n",
    "As we can see, the maximum number of consumers in a consumer group should not exceed the number of partitions, else we would have idle consumers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a55f9b-cb00-419d-82d9-36f9b005d540",
   "metadata": {},
   "source": [
    "## Rebalancing\n",
    "Partitions assigned to consumers in a consumer group need to revisited when:\n",
    "- a consumer fails (no heartbeat)\n",
    "- a consumer leaves a group (by calling `close()`)\n",
    "- a new consumer joins (first call to `poll()`)\n",
    "- new partition gets added\n",
    "\n",
    "Moving partition ownership from one consumer to another is called a *rebalance*. There are two ways to perform rebalancing:  \n",
    "- **Eager Rebalancing:** all consumers temporarily stop consuming, give up their ownership of partitions, leave the consumer group, rejoin the consumer group and finally get reassigned a new partition.\n",
    "- **Cooperative Rebalancing:** this is similar to eager rebalancing, except for only a subset of partitions are considered for rebalancing. The advantage is that it avoids *stop-the-world* unavailability of the eager method.\n",
    "\n",
    "### Group Coordinator and Group Leader\n",
    "The Group Coordinator is a dedicated Kafka broker that is responsible for managing the state of a specific consumer group. The first consumer to join a consumer group (by sending a `JoinGroup` request) becomes the group leader.\n",
    "\n",
    "Group leader receives list of all consumers in a consumer group from the group coordinator and decides the partition ownership for all consumers. Once the partition assignment is decided it is sent to group coordinator which then propagates it to rest of the consumers in the consumer group. The consumers have to send heartbeats to the group coordinator to be considered active."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e52eb5-95ac-46ee-a509-b9a8cbee7900",
   "metadata": {},
   "source": [
    "## Constructing Consumer\n",
    "We instantiate an instance of `KafkaConsumer` as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569b77a-6733-48b6-8c79-ba7509eda40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Properties props = new Properties();\n",
    "props.put(\"bootstrap.servers\", \"broker1:9092,broker2:9092\");\n",
    "props.put(\"group.id\", \"CountryCounter\"); // This is the consumer group (not mandatory)\n",
    "props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n",
    "props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");\n",
    "\n",
    "KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a2e094-584c-4ea7-a8ba-853d22afb607",
   "metadata": {},
   "source": [
    "If `group.id` is not specified, the consumer will exhibit the following behaviours:\n",
    "- no offset committing: if the consumer stops and restarts, it will not remember where it left off\n",
    "- no load balancing: every ungrouped consumer will read all available partitions\n",
    "\n",
    "Thus it is not common to not specify `group.id`.\n",
    "\n",
    "Full list available [here](https://kafka.apache.org/documentation.html#newconsumerconfigs). Some important ones are: \n",
    "\n",
    "| Property            | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
    "|---------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| fetch.min.bytes     | minimum amount of data that it wants to receive from the broker when fetching records. If a broker receives a request for records from a consumer but the new records amount to fewer bytes, the broker will wait until more messages are available before sending the records back to the consumer.                                                                                                                                                                                                                       |\n",
    "| fetch.max.wait.ms   | broker will not wait longer than this duration before sending a response back, even if the required fetch.min.bytes has not been met. By default, Kafka will wait up to 500 ms.                                                                                                                                                                                                                                                                                                                                            |\n",
    "| session.timeout.ms  | amount of time a consumer can be out of contact with the brokers while still considered alive, defaults to 10 seconds. Its value should be greater than `heartbeat.interval.ms` which controls heartbeat rate.                                                                                                                                                                                                                                                                                                             |\n",
    "| max.poll.interval.ms| maximum amount of time a consumer can spend between calls to `poll()` before it is considered dead and a rebalance is triggered. Defaults to 5 minutes. This is helpful in case consumer thread gets deadlocked or stuck while heartbeat thread keeps on sending heartbeats.                                                                                                                                                                                                                                               |\n",
    "| auto.offset.reset   | Controls the behavior of the consumer when it starts reading a partition for which it doesn’t have a committed offset or if the committed offset it has is invalid (the consumer was down for so long that the record with that offset was already aged out of the broker). latest: default value and means that lacking a valid offset, the consumer will start reading from the newest records earliest: lacking a valid offset, the consumer will read all the data in the partition, starting from the very beginning. |\n",
    "| max.poll.records    | maximum number of records that a single call to `poll()` will return.                                                                                                                                                                                                                                                                                                                                                                                                                                                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2582c35a-5b21-4ad3-a31e-faf6b7892be8",
   "metadata": {},
   "source": [
    "## Consuming Message\n",
    "We start by subscribing to a topic (or multiple topics):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab21e4f-0951-4a59-b3cb-2dca26792eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer.subscribe(Collections.singletonList(\"customerCountries\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c87cd4-05ae-4fc3-9d3d-971fc3d149f9",
   "metadata": {},
   "source": [
    "By invoking `subscribe`, the consumer:\n",
    "- indicates that it wants to consume from the given list of topics\n",
    "- shows intent to join consumer group specified by `group.id` property\n",
    "\n",
    "At this point:\n",
    "- no partitions are assigned yet\n",
    "- no data is fetched\n",
    "- no offsets are read\n",
    "\n",
    "**Poll Loop:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74933ef-5fe7-459c-90aa-45be53118c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Polling for messages - handles all details of coordination,\n",
    "// partition rebalances, heartbeats, and data fetching\n",
    "try {\n",
    "    while (true) {\n",
    "        // argument is timeout interval and controls how long poll() will block if data is not available\n",
    "        // in the consumer buffer. Set to 0, poll() will return immediately\n",
    "        ConsumerRecords<String, String> records = consumer.poll(100);\n",
    "        \n",
    "        // Poll returns multiple records. Each record contains the topic and partition the\n",
    "        // record came from, the offset of the record within the partition, and of course the\n",
    "        // key and the value of the record.\n",
    "        for (ConsumerRecord<String, String> record : records) {\n",
    "            LOGGER.debug(\"topic = {}, partition = {}, offset = {}, customer = {}, country = {}\",\n",
    "                record.topic(), record.partition(), record.offset(), record.key(), record.value());\n",
    "            \n",
    "            int updatedCount = 0;\n",
    "            if (custCountryMap.containsKey(record.value())) {\n",
    "                updatedCount = custCountryMap.get(record.value()) + 1;\n",
    "            }\n",
    "            \n",
    "            custCountryMap.put(record.value(), updatedCount);\n",
    "        }\n",
    "    }\n",
    "} finally {\n",
    "    // This will close the network connections and sockets.\n",
    "    // Also retrigger rebalance immediately.\n",
    "    consumer.close();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf52af-11f5-44d7-b224-a8480a5083b4",
   "metadata": {},
   "source": [
    "`poll` does a lot more than getting messages. The first call to `poll`:\n",
    "  \n",
    "  **Group Joining and Coordination:**\n",
    "  - the consumer contacts a bootstrap broker and discovers the *group coordinator* for `group.id`\n",
    "  - sends a `JoinGroup` request. If there are no consumers in the group, the current consumer becomes the *group leader*\n",
    "  - the group coordinator detects that a new member has joined and initiates a rebalance. This means:\n",
    "      - all consumers in the group temporarily stop consumption\n",
    "      - the group leader runs the partition assignment strategy and hands it over to group coordinator\n",
    "      - the group coordinator distributes the assignment plan to all consumers\n",
    "  - the consumer obtains its partition(s)\n",
    "  \n",
    "  **Offset Retrieval:**\n",
    "  - the consumer sends a request to the broker to fetch the last committed offset for its `group.id` to know where to start from\n",
    "  - when the partitions and starting offsets are determined, the consumer retrieves the message\n",
    "\n",
    "Subsequent `poll()` invocations:\n",
    "- send heartbeat\n",
    "- handle rebalance\n",
    "- fetch message\n",
    "- commit offset (described later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956ef346-3aeb-4159-94ba-fe212a23a497",
   "metadata": {},
   "source": [
    "## Commits\n",
    "Every message stored in a broker has a sequential, immutable unique identifier associated with it - the message offset. It is essentially the index of the record in the partition. Whenever we poll, we intend to read new messages that were not processed before - continue from the last offset that was read.\n",
    "\n",
    "In Kafka, it is the consumer's job to store the last read commit for a specific partition back to a designated location on the Kafka cluster. This process is called as *committing offset*. This process is critical because it tells Kafka: \"I have successfully finished processing all messages up to this point. If I restart, begin sending me messages starting from the next offset\".\n",
    "\n",
    "Kafka has a designated topic `__consumer_offsets` that stores offsets:\n",
    "\n",
    "<img src=\"images/consumer_offsets.png\" />\n",
    "\n",
    "The commit process looks like:\n",
    "1. Consumer processes records up to a certain offset (e.g., 1000).\n",
    "2. Consumer sends an `OffsetCommitRequest` (committing offset 1001) to its group coordinator.\n",
    "3. The group coordinator receives the request and writes a record to the `__consumer_offsets` topic with the key `⟨my-group-id,orders-topic,5⟩` and the value 1001.\n",
    "4. Later, when the consumer restarts, it asks the group coordinator for the last committed offset, and the coordinator looks up the most recent value for that key, finds 1001, and tells the consumer to start reading from that point.\n",
    "\n",
    "Consumer internally maintains offset number it has consumed. And uses that number while getting new message using `poll`. Committed offsets are only needed when the first `poll` call is made or when rebalance happens and partitions are redistributed. Therefore committing offsets is like a checkpoint for where you want the app to return should it fail (or shuts down and another consumer has to continue).\n",
    "\n",
    "Since the act of committing offsets is independent from fetching messages using the internal offset number, it can lead to issue if those two are not in sync:  \n",
    "\n",
    "<img src=\"images/duplicate_processing.png\" />\n",
    "\n",
    "### Committing Offsets\n",
    "There are variety of ways to commit offset:  \n",
    "- **Automatic Commits:** configure `enable.auto.commit=true`, then every five seconds the consumer will commit the largest offset your client received from poll(). Just like everything else in the consumer, the automatic commits are driven by the `poll` loop. Whenever we `poll`, the consumer checks if it is time to commit, and if it is, it will commit the offsets it returned in the last `poll`.\n",
    "\n",
    "- **Commit Synchronously:** by setting `auto.commit.offset=false`, offsets will only be committed when the application explicitly chooses to do so (using `commitSync()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29390ad-aadb-412e-b8d6-734b5f702ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "while (true) {\n",
    "    ConsumerRecords<String, String> records = consumer.poll(100);\n",
    "    for (ConsumerRecord<String, String> record : records) {\n",
    "        System.out.printf(\"topic = %s, partition = %s, offset = %d, customer = %s, country = %s\\n\",\n",
    "            record.topic(), record.partition(), record.offset(), record.key(), record.value());\n",
    "    }\n",
    "    \n",
    "    try {\n",
    "        consumer.commitSync(); // commits the latest offset returned by the last poll()\n",
    "    } catch (CommitFailedException e) {\n",
    "        log.error(\"commit failed\", e)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda02f3-d3cd-4430-9161-1b92149daddb",
   "metadata": {},
   "source": [
    "- **Commit Asynchronously:** to improve throughput of the application. The drawback is that while `commitSync()` will retry the commit until it either succeeds or encounters a nonretriable failure, `commitAsync()` will not retry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f1ada8-d899-4efc-9ca4-7b7f9fa4f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "while (true) {\n",
    "    ConsumerRecords<String, String> records = consumer.poll(100);\n",
    "    for (ConsumerRecord<String, String> record : records) {\n",
    "        System.out.printf(\"topic = %s, partition = %s, offset = %d, customer = %s, country = %s\\n\",\n",
    "        record.topic(), record.partition(), record.offset(), record.key(), record.value());\n",
    "    }\n",
    "    \n",
    "    consumer.commitAsync();\n",
    "}\n",
    "\n",
    "// Commit async supports callback\n",
    "consumer.commitAsync(new OffsetCommitCallback() {\n",
    "    public void onComplete(Map<TopicPartition, OffsetAndMetadata> offsets, Exception exception) {\n",
    "    if (e != null)\n",
    "        log.error(\"Commit failed for offsets {}\", offsets, e);\n",
    "    }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54387080-0ef8-4564-bc0c-3903ed49de37",
   "metadata": {},
   "source": [
    "A simple pattern to get commit order right for asynchronous retries is to use a monotonically increasing sequence number. Increase the sequence number every time we commit and add the sequence number at the time of the commit to the `commitAsync` callback. When we’re getting ready to send a retry, check if the commit sequence number the callback got is equal to the instance variable; if it is, there was no newer commit and it is safe to retry. If the instance sequence number is higher, don’t retry because a newer commit was already sent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2e2cb-ed5d-4eda-a6f3-07677deb4ab6",
   "metadata": {},
   "source": [
    "- **Combined:** a common pattern is to combine `commitAsync()` with `commitSync()` just before shutdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aedacb-6ece-4770-8af5-2940bce5505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try {\n",
    "    while (!closing) {\n",
    "        ConsumerRecords<String, String> records = consumer.poll(100);\n",
    "        for (ConsumerRecord<String, String> record : records) {\n",
    "            System.out.printf(\"topic = %s, partition = %s, offset = %d, customer = %s, country = %s\\n\",\n",
    "            record.topic(), record.partition(), record.offset(), record.key(), record.value());\n",
    "        }\n",
    "        consumer.commitAsync();\n",
    "    }\n",
    "    consumer.commitSync();\n",
    "} catch (Exception e) {\n",
    "    log.error(\"Unexpected error\", e);\n",
    "} finally {\n",
    "    consumer.close();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb362a7d-d4d0-4a7e-90e3-9bf928f78659",
   "metadata": {},
   "source": [
    "What if the number of records received in poll is high and we want to commit in between processing? The following code commits after every 1000 records processed using an alternate form of `commitAsync`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b182634-05f6-4014-b3d5-47abc90a0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "private Map<TopicPartition, OffsetAndMetadata> currentOffsets = new HashMap<>();\n",
    "int count = 0;\n",
    "// ...\n",
    "\n",
    "while (true) {\n",
    "    ConsumerRecords<String, String> records = consumer.poll(100);\n",
    "    for (ConsumerRecord<String, String> record : records) {\n",
    "        System.out.printf(\"topic = %s, partition = %s, offset = %d, customer = %s, country = %s\\n\",\n",
    "            record.topic(), record.partition(), record.offset(), record.key(), record.value());\n",
    "    \n",
    "        currentOffsets.put(new TopicPartition(record.topic(), record.partition()), \n",
    "            new OffsetAndMetadata(record.offset()+1, \"no metadata\"));\n",
    "        if (count % 1000 == 0)\n",
    "            consumer.commitAsync(currentOffsets, null);\n",
    "        count++;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96ec3b-6fd1-4db6-abba-54a339d919e8",
   "metadata": {},
   "source": [
    "### Reading Specific Commits\n",
    "If we want to start reading all messages from the beginning of the partition, or we want to skip all the way to the end of the partition and start consuming only new messages, there are APIs specifically for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947e44d2-e814-442c-a9d0-789d3cb85a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Collection because a consumer may be assigned multiple partitions\n",
    "public void seekToBeginning(Collection<TopicPartition> partitions);\n",
    "\n",
    "public void seekToEnd(Collection<TopicPartition> partitions);\n",
    "\n",
    "public void seek(TopicPartition partition, OffsetAndMetadata offsetAndMetadata);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1302b3f3-3d53-4bb1-b60a-f39c9b251fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Read logs from the beginning\n",
    "final String TOPIC = \"logs\";\n",
    "\n",
    "consumer.subscribe(List.of(TOPIC));\n",
    "consumer.poll(0); // get partition assignment\n",
    "\n",
    "consumer.seekToBeginning(consumer.assignment()); // goto start for all assigned partitions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "21.0.2+13-58"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
