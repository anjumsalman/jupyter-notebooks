{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron model\n",
    "The image below represents a neuron model:\n",
    "![Neuron Model](https://i.imgur.com/tJhYe2p.gif)\n",
    "If we represent our input x and coefficient $\\theta$ as:\n",
    "\n",
    "  \n",
    "$$x = \\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "  \n",
    "$$\\theta = \\begin{bmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And\n",
    "\n",
    "  \n",
    "$$h_\\theta(x) = \\frac{1}{1 + e^{\\theta^T x}}$$\n",
    "\n",
    "  \n",
    "## Neural Network\n",
    "![Neural Network](https://i.imgur.com/Or9y7Kg.gif)\n",
    "Below $a_i^j$ means ith activation unit of jth layer and $\\Theta^j$ is matrix of weights controlling function mapping from layer $j$ to $j+1$\n",
    "\n",
    "  \n",
    "$$a_1^2 = g(\\Theta^1_{10}x_0 + \\Theta^1_{11}x_1 + \\Theta^1_{12}x_2 + \\Theta^1_{13}x_3) = g(z_1^2)$$\n",
    "  \n",
    "$$a_2^2 = g(\\Theta^1_{20}x_0 + \\Theta^1_{21}x_1 + \\Theta^1_{22}x_2 + \\Theta^1_{23}x_3) = g(z_2^2)$$\n",
    "  \n",
    "$$a_3^2 = g(\\Theta^1_{30}x_0 + \\Theta^1_{31}x_1 + \\Theta^1_{32}x_2 + \\Theta^1_{33}x_3) = g(z_3^2)$$\n",
    "  \n",
    "$$h_{\\Theta}(x)=a_1^3=g(\\Theta^2_{10}a_0^2 + \\Theta^2_{11}a_1^2 + \\Theta^2_{12}a_2^2 + \\Theta^2_{13}a_3^2)$$\n",
    "  \n",
    "We can see that if a layer has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\\Theta^j$ will be of dimension $s_{j+1}\\times(s_j+1)$.\n",
    "  \n",
    "Not let us consider the matrices:\n",
    "  \n",
    "$$x = \\begin{bmatrix}\n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "  \n",
    "$$z^2 = \\begin{bmatrix}\n",
    "z_1^2 \\\\\n",
    "z_2^2 \\\\\n",
    "z_3^2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\\Theta^1 = \\begin{bmatrix}\n",
    "\\Theta^1_{10} & \\Theta^1_{11} & \\Theta^1_{12} & \\Theta^1_{13}\\\\\n",
    "\\Theta^1_{20} & \\Theta^1_{21} & \\Theta^1_{22} & \\Theta^1_{23}\\\\\n",
    "\\Theta^1_{30} & \\Theta^1_{31} & \\Theta^1_{32} & \\Theta^1_{33}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "  \n",
    "This leads to\n",
    "$$z^2 = \\Theta^1x$$\n",
    "$$a^2 = g(z^2)$$\n",
    "  \n",
    "Consider $a_0^2 = 1$.\n",
    "  \n",
    "For **multiclass neural network** the output layer contains more than one unit, and the output in this case therefore is a vector.\n",
    "\n",
    "## Cost Function for Neural Network\n",
    "Let $L$ = total number of layers, $s_l$ = number of units or neurons in each layer and $K$ = number of classes. The cost function for neural network is similar to that of logistic regression\n",
    "  \n",
    "$$J(\\Theta) = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}[y_k^ilog(h_{\\theta}(x^i))_k + (1-y_k^i)log(1-h_{\\theta}(x^i))_k] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_l+1}(\\Theta_{j,i}^{l})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
